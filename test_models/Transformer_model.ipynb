{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Descriptors\n",
    "import os\n",
    "os.chdir(\"..\")\n",
    "os.chdir(\"Preprocessing\")\n",
    "from preprocessing import *\n",
    "import selfies as sf\n",
    "\n",
    "#Ensemble learning and random forest\n",
    "\n",
    "df = preprocessing(\"C:\\\\Users\\Gilbert\\Documents\\BCB_Research\\Kcat_Benchmark_ML_Models\\Data\\kcat_transferase.csv\")\n",
    "encoded_df = pd.read_csv(\"C:\\\\Users\\Gilbert\\Documents\\BCB_Research\\Kcat_Benchmark_ML_Models\\Data\\encoded_amino.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = encoded_df\n",
    "y = np.log10(df[\"Kcat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshape successful, new shape: (4136, 2511, 21)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    x_new = np.reshape(x, (4136, 2511, 21))\n",
    "    print(\"Reshape successful, new shape:\", x_new.shape)\n",
    "except ValueError as e:\n",
    "    print(\"Reshape failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train , y_test = train_test_split(x_new, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 0 0 ... 0 0 0]\n",
      "  [1 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 1]\n",
      "  [0 0 0 ... 0 0 1]\n",
      "  [0 0 0 ... 0 0 1]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 1 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 1]\n",
      "  [0 0 0 ... 0 0 1]\n",
      "  [0 0 0 ... 0 0 1]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 1]\n",
      "  [0 0 0 ... 0 0 1]\n",
      "  [0 0 0 ... 0 0 1]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [1 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 1]\n",
      "  [0 0 0 ... 0 0 1]\n",
      "  [0 0 0 ... 0 0 1]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 1]\n",
      "  [0 0 0 ... 0 0 1]\n",
      "  [0 0 0 ... 0 0 1]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [1 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 1]\n",
      "  [0 0 0 ... 0 0 1]\n",
      "  [0 0 0 ... 0 0 1]]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 100, 20)]         0         \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 100, 128)          2688      \n",
      "                                                                 \n",
      " model_10 (Functional)       (None, 100, 128)          793088    \n",
      "                                                                 \n",
      " global_average_pooling1d_1  (None, 128)               0         \n",
      "  (GlobalAveragePooling1D)                                       \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_55 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 820609 (3.13 MB)\n",
      "Trainable params: 820609 (3.13 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_13 (InputLayer)       [(None, 2511, 21)]        0         \n",
      "                                                                 \n",
      " dense_56 (Dense)            (None, 2511, 128)         2816      \n",
      "                                                                 \n",
      " model_16 (Functional)       (None, 2511, 128)         793088    \n",
      "                                                                 \n",
      " global_average_pooling1d_2  (None, 128)               0         \n",
      "  (GlobalAveragePooling1D)                                       \n",
      "                                                                 \n",
      " dense_81 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_26 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_82 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_83 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 820737 (3.13 MB)\n",
      "Trainable params: 820737 (3.13 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def scaled_dot_product_attention(q, k, v):\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "    d_k = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(d_k)\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    return output\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        assert d_model % self.num_heads == 0\n",
    "        self.depth = d_model // self.num_heads\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        scaled_attention = scaled_dot_product_attention(q, k, v)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "        output = self.dense(concat_attention)\n",
    "        return output\n",
    "\n",
    "def get_positional_encoding(sequence_length, d_model):\n",
    "    angles = 1 / np.power(10000, (2 * (np.arange(d_model)[np.newaxis, :] // 2)) / np.float32(d_model))\n",
    "    angle_rads = np.arange(sequence_length)[:, np.newaxis] * angles\n",
    "    sines = np.sin(angle_rads[:, 0::2])\n",
    "    cosines = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = np.concatenate([sines, cosines], axis=-1)\n",
    "    pos_encoding = pos_encoding[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "\n",
    "def encoder_layer(units, d_model, num_heads, dropout_rate):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model))\n",
    "    mha = MultiHeadAttention(d_model, num_heads)(inputs, inputs, inputs)\n",
    "    dropout1 = tf.keras.layers.Dropout(dropout_rate)(mha)\n",
    "    norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs + dropout1)\n",
    "    ffn_output = tf.keras.layers.Dense(units, activation='relu')(norm1)\n",
    "    ffn_output = tf.keras.layers.Dense(d_model)(ffn_output)\n",
    "    dropout2 = tf.keras.layers.Dropout(dropout_rate)(ffn_output)\n",
    "    outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(norm1 + dropout2)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "def encoder(sequence_length, d_model, num_heads, dropout_rate, num_layers, units):\n",
    "    inputs = tf.keras.Input(shape=(sequence_length, d_model))\n",
    "    pos_encoding = get_positional_encoding(sequence_length, d_model)\n",
    "    x = inputs + pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "    for _ in range(num_layers):\n",
    "        x = encoder_layer(units, d_model, num_heads, dropout_rate)(x)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "def build_model(sequence_length, one_hot_dim, num_layers, units, d_model, num_heads, dropout_rate):\n",
    "    inputs = tf.keras.Input(shape=(sequence_length, one_hot_dim))\n",
    "    x = tf.keras.layers.Dense(d_model)(inputs)\n",
    "    x = encoder(sequence_length, d_model, num_heads, dropout_rate, num_layers, units)(x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "    outputs = tf.keras.layers.Dense(1)(x)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Hyperparameters\n",
    "SEQUENCE_LENGTH = 2511  # Changed from 100 to 2511\n",
    "ONE_HOT_DIM = 21  \n",
    "NUM_LAYERS = 4\n",
    "UNITS = 512\n",
    "D_MODEL = 128\n",
    "NUM_HEADS = 8\n",
    "DROPOUT_RATE = 0.1\n",
    "\n",
    "model = build_model(SEQUENCE_LENGTH, ONE_HOT_DIM, NUM_LAYERS, UNITS, D_MODEL, NUM_HEADS, DROPOUT_RATE)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\Gilbert\\anaconda3\\envs\\bcb_2\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Gilbert\\anaconda3\\envs\\bcb_2\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Gilbert\\anaconda3\\envs\\bcb_2\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\Gilbert\\anaconda3\\envs\\bcb_2\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1126, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\Gilbert\\anaconda3\\envs\\bcb_2\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\Gilbert\\anaconda3\\envs\\bcb_2\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_11\" is incompatible with the layer: expected shape=(None, 100, 20), found shape=(None, 2511, 21)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Gilbert\\Documents\\BCB_Research\\Kcat_Benchmark_ML_Models\\test_models\\Transformer_model.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Gilbert/Documents/BCB_Research/Kcat_Benchmark_ML_Models/test_models/Transformer_model.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmean_squared_error\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mmean_absolute_error\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Gilbert/Documents/BCB_Research/Kcat_Benchmark_ML_Models/test_models/Transformer_model.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(x_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Gilbert\\anaconda3\\envs\\bcb_2\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filegd_nyrgs.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\Gilbert\\anaconda3\\envs\\bcb_2\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Gilbert\\anaconda3\\envs\\bcb_2\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Gilbert\\anaconda3\\envs\\bcb_2\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\Gilbert\\anaconda3\\envs\\bcb_2\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1126, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\Gilbert\\anaconda3\\envs\\bcb_2\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\Gilbert\\anaconda3\\envs\\bcb_2\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_11\" is incompatible with the layer: expected shape=(None, 100, 20), found shape=(None, 2511, 21)\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
    "history = model.fit(x_train, y_train, epochs=100, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test MAE: {test_mae}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bcb_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
