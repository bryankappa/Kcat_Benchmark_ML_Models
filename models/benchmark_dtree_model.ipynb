{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, RDKFingerprint\n",
    "import os\n",
    "os.chdir(\"..\")\n",
    "os.chdir(\"Preprocessing\")\n",
    "from preprocessing import *\n",
    "import selfies as sf\n",
    "\n",
    "#Ensemble learning and random forest\n",
    "\n",
    "df = preprocessing(\"C:\\\\Users\\Gilbert\\Documents\\BCB_Research\\Kcat_Benchmark_ML_Models\\models\\kcat_transferase.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix later this is for selfies\n",
    "\n",
    "# df[\"Compound\"] = [sf.encoder(i) for i in df[\"Compound\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = df[\"Compound\"]\n",
    "\n",
    "# alphabet = sf.get_alphabet_from_selfies(dataset)\n",
    "# alphabet.add(\"[nop]\")\n",
    "\n",
    "# # Manually add other common SELFIES symbols if needed\n",
    "# additional_symbols = ['[=]', '[#]', '[+]', '.']  # Add more symbols if necessary\n",
    "# alphabet.update(additional_symbols)\n",
    "\n",
    "# alphabet = list(sorted(alphabet))\n",
    "\n",
    "# # Determine the maximum length for padding\n",
    "# pad_to_len = max(sf.len_selfies(s) for s in dataset)\n",
    "\n",
    "# # Create symbol-to-index mapping\n",
    "# symbol_to_idx = {s: i for i, s in enumerate(alphabet)}\n",
    "\n",
    "# # Initialize empty lists for labels and one-hot encodings\n",
    "# labels = []\n",
    "# one_hot_encodings = []\n",
    "\n",
    "# # Convert each SELFIES string to label and one-hot encoding\n",
    "\n",
    "# for selfies in dataset:\n",
    "#     label, one_hot = sf.selfies_to_encoding(\n",
    "#         selfies=selfies,\n",
    "#         vocab_stoi=symbol_to_idx,\n",
    "#         pad_to_len=pad_to_len,\n",
    "#         enc_type=\"both\"\n",
    "#     )\n",
    "#     labels.append(label)\n",
    "#     one_hot_encodings.append(one_hot)\n",
    "\n",
    "# # Convert lists to DataFrame columns\n",
    "# data_self = {'Label': labels, 'OneHotEncoding': one_hot_encodings}\n",
    "# data_selfies = pd.DataFrame(data_self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57324960"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selfies = np.array(data_selfies[\"OneHotEncoding\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = df.copy()\n",
    "\n",
    "d.head()\n",
    "\n",
    "data = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the one-hot encodings\n",
    "one_hot_encodings = np.array([np.array(row) for row in one_hot_encodings])\n",
    "# Reshape the one-hot encodings to (4136, 220*63)\n",
    "reshaped_encodings = one_hot_encodings.reshape(one_hot_encodings.shape[0], -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4136, 13860)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3163    78300.00000\n",
      "3851    32000.00000\n",
      "2141    14000.00000\n",
      "2142    14000.00000\n",
      "2252    14000.00000\n",
      "662      8050.00000\n",
      "810      7000.00000\n",
      "4061     6666.66667\n",
      "2748     6600.00000\n",
      "937      6004.00000\n",
      "3327     6000.00000\n",
      "936      5694.00000\n",
      "1187     5460.00000\n",
      "929      5252.00000\n",
      "925      5190.00000\n",
      "1160     5086.00000\n",
      "951      4500.00000\n",
      "935      4412.00000\n",
      "1181     4080.00000\n",
      "811      4000.00000\n",
      "934      3980.00000\n",
      "3266     3869.00000\n",
      "933      3595.00000\n",
      "491      3580.00000\n",
      "494      3580.00000\n",
      "931      3453.00000\n",
      "492      3450.00000\n",
      "930      3324.00000\n",
      "928      3217.00000\n",
      "2992     3204.00000\n",
      "Name: Kcat, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Replace '5' with the number of largest values you want to view\n",
    "top_n = 30\n",
    "\n",
    "# Use n largest values in 'Kcat' column\n",
    "largest_kcat_values = data.nlargest(top_n, \"Kcat\")[\"Kcat\"]\n",
    "\n",
    "print(largest_kcat_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "#lets encode the data using label encoder \n",
    "label_encoder = LabelEncoder()\n",
    "data[\"EC_number\"] = label_encoder.fit_transform(data[\"EC_number\"])\n",
    "data[\"Species\"] = label_encoder.fit_transform(data[\"Species\"])\n",
    "\n",
    "amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "amino_to_index = {amino: i for i, amino in enumerate(amino_acids)}\n",
    "\n",
    "# Convert amino acid sequences to one-hot encoding\n",
    "def convert_to_one_hot(sequence, max_length):\n",
    "    one_hot_sequence = np.zeros((max_length, len(amino_acids)))\n",
    "    \n",
    "    for i, amino in enumerate(sequence):\n",
    "        if amino in amino_to_index:\n",
    "            index = amino_to_index[amino]\n",
    "            one_hot_sequence[i, index] = 1\n",
    "            \n",
    "    return one_hot_sequence.flatten()\n",
    "\n",
    "# Determine the maximum sequence length\n",
    "max_sequence_length = max(len(seq) for seq in data[\"Amino_encoding\"])\n",
    "\n",
    "# Apply the conversion to the DataFrame column\n",
    "data[\"Amino\"] = data[\"Amino_encoding\"].apply(lambda seq: convert_to_one_hot(seq, max_sequence_length)).tolist()\n",
    "\n",
    "# convert compound name into numbers.\n",
    "compound = data[\"Compound\"]\n",
    "\n",
    "data[\"smiles\"] = [Chem.MolFromSmiles(smiles) for smiles in compound]\n",
    "\n",
    "mol = data[\"smiles\"]\n",
    "\n",
    "data[\"Fingerprint_rdk\"] = [RDKFingerprint(i) for i in mol]\n",
    "\n",
    "# Apply logarithmic transformation to 'Kcat'\n",
    "data[\"Kcat\"] = np.log10(data[\"Kcat\"])  # Applying log(x + 1) to handle zeros\n",
    "\n",
    "# # Normalize the target variable using Min-Max scaling\n",
    "# scaler = MinMaxScaler()\n",
    "# data[\"Kcat_normalized\"] = scaler.fit_transform(data[[\"Kcat\"]])\n",
    "\n",
    "data.drop(columns=[\"Compound\", \"Compound_name\", \"Amino_encoding\", \"smiles\", \"unit\"], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features = data.copy()\n",
    "\n",
    "data_features.drop(columns=[\"Kcat\", \"Amino\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EC_number</th>\n",
       "      <th>Species</th>\n",
       "      <th>Kcat</th>\n",
       "      <th>Amino</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>-1.387216</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>0.008600</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>-1.080922</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>-1.425969</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>81</td>\n",
       "      <td>-0.420216</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   EC_number  Species      Kcat  \\\n",
       "0          0       99 -1.387216   \n",
       "1          0       99  0.008600   \n",
       "2          0       99 -1.080922   \n",
       "3          1       39 -1.425969   \n",
       "4          1       81 -0.420216   \n",
       "\n",
       "                                               Amino  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and split the data\n",
    "# EC_number, Species, Amino, and fingerprint_rdk\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# x_features = np.array(data_features['Fingerprint_rdk'].tolist())\n",
    "# # y = np.array(data[\"Kcat_normalized\"])\n",
    "\n",
    "# x_features = pd.DataFrame(x_features)\n",
    "\n",
    "x = np.array(data_features['Fingerprint_rdk'].tolist())\n",
    "y = data[\"Kcat\"].values\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (3308,)\n",
      "y_train shape: (3308,)\n",
      "x_train data type: <class 'pandas.core.series.Series'>\n",
      "y_train data type: <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "\n",
    "# Check data types\n",
    "print(\"x_train data type:\", type(x_train))\n",
    "print(\"y_train data type:\", type(y_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a real number, not 'list'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# this initialization of the regression model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m rf_regressor \u001b[39m=\u001b[39m RandomForestRegressor(n_estimators\u001b[39m=\u001b[39m\u001b[39m125\u001b[39m, max_depth\u001b[39m=\u001b[39m\u001b[39m15\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m rf_regressor\u001b[39m.\u001b[39;49mfit(x_train, y_train)\n\u001b[0;32m      6\u001b[0m y_pred \u001b[39m=\u001b[39m rf_regressor\u001b[39m.\u001b[39mpredict(x_test)\n",
      "File \u001b[1;32mc:\\Users\\Gilbert\\anaconda3\\envs\\bcb\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Gilbert\\anaconda3\\envs\\bcb\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:348\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[39mif\u001b[39;00m issparse(y):\n\u001b[0;32m    347\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 348\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    349\u001b[0m     X, y, multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mDTYPE\n\u001b[0;32m    350\u001b[0m )\n\u001b[0;32m    351\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    352\u001b[0m     sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[1;32mc:\\Users\\Gilbert\\anaconda3\\envs\\bcb\\Lib\\site-packages\\sklearn\\base.py:621\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    619\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    620\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 621\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[0;32m    622\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    624\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\Gilbert\\anaconda3\\envs\\bcb\\Lib\\site-packages\\sklearn\\utils\\validation.py:1147\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1142\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1143\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1144\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1145\u001b[0m     )\n\u001b[1;32m-> 1147\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m   1148\u001b[0m     X,\n\u001b[0;32m   1149\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[0;32m   1150\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[0;32m   1151\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   1152\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[0;32m   1153\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m   1154\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[0;32m   1155\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[0;32m   1156\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[0;32m   1157\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[0;32m   1158\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[0;32m   1159\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m   1160\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1161\u001b[0m )\n\u001b[0;32m   1163\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[0;32m   1165\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mc:\\Users\\Gilbert\\anaconda3\\envs\\bcb\\Lib\\site-packages\\sklearn\\utils\\validation.py:917\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    915\u001b[0m         array \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(array, dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    916\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 917\u001b[0m         array \u001b[39m=\u001b[39m _asarray_with_order(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype, xp\u001b[39m=\u001b[39;49mxp)\n\u001b[0;32m    918\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[0;32m    919\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    920\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    921\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Gilbert\\anaconda3\\envs\\bcb\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:380\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    378\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39marray(array, order\u001b[39m=\u001b[39morder, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m    379\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 380\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39masarray(array, order\u001b[39m=\u001b[39morder, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m    382\u001b[0m \u001b[39m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[39m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39masarray(array)\n",
      "File \u001b[1;32mc:\\Users\\Gilbert\\anaconda3\\envs\\bcb\\Lib\\site-packages\\pandas\\core\\series.py:917\u001b[0m, in \u001b[0;36mSeries.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    870\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    871\u001b[0m \u001b[39mReturn the values as a NumPy array.\u001b[39;00m\n\u001b[0;32m    872\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    914\u001b[0m \u001b[39m      dtype='datetime64[ns]')\u001b[39;00m\n\u001b[0;32m    915\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    916\u001b[0m values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values\n\u001b[1;32m--> 917\u001b[0m arr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(values, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    918\u001b[0m \u001b[39mif\u001b[39;00m using_copy_on_write() \u001b[39mand\u001b[39;00m astype_is_view(values\u001b[39m.\u001b[39mdtype, arr\u001b[39m.\u001b[39mdtype):\n\u001b[0;32m    919\u001b[0m     arr \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mview()\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "# this initialization of the regression model\n",
    "rf_regressor = RandomForestRegressor(n_estimators=125, max_depth=15, random_state=42)\n",
    "\n",
    "rf_regressor.fit(x_train, y_train)\n",
    "\n",
    "y_pred = rf_regressor.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "estimators = np.arange(10, 200, 10)\n",
    "scores = []\n",
    "for n in estimators:\n",
    "    rf_regressor.set_params(n_estimators=n)\n",
    "    rf_regressor.fit(x_train, y_train)\n",
    "    scores.append(rf_regressor.score(x_test, y_test))\n",
    "plt.title(\"Effect of n_estimators\")\n",
    "plt.xlabel(\"n_estimator\")\n",
    "plt.ylabel(\"score\")\n",
    "plt.plot(estimators, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "\n",
    "print('MAE: ', mean_absolute_error(y_test, y_pred))\n",
    "print('MSE: ', mean_squared_error(y_test, y_pred)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Kcat_normalized\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# helps finding the optimal hyperparameters\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    # Add more hyperparameters and values\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(RandomForestRegressor(random_state=42), param_grid=param_grid, cv=3)\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "best_rf_regressor = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_rf_regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random forest regressor\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "\n",
    "regressor = RandomForestRegressor(n_estimators=150, max_depth=15, random_state=42)\n",
    "\n",
    "# Create learning curve data\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    regressor, x, y, cv=5, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10)\n",
    ")\n",
    "\n",
    "# Calculate mean and standard deviation of scores\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Plot learning curve\n",
    "plt.figure()\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.xlabel(\"Training Examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.grid()\n",
    "\n",
    "plt.fill_between(\n",
    "    train_sizes,\n",
    "    train_scores_mean - train_scores_std,\n",
    "    train_scores_mean + train_scores_std,\n",
    "    alpha=0.1,\n",
    "    color=\"r\",\n",
    ")\n",
    "plt.fill_between(\n",
    "    train_sizes,\n",
    "    test_scores_mean - test_scores_std,\n",
    "    test_scores_mean + test_scores_std,\n",
    "    alpha=0.1,\n",
    "    color=\"g\",\n",
    ")\n",
    "plt.plot(\n",
    "    train_sizes, train_scores_mean, \"o-\", color=\"r\", label=\"Training score\"\n",
    ")\n",
    "plt.plot(\n",
    "    train_sizes, test_scores_mean, \"o-\", color=\"g\", label=\"Cross-validation score\"\n",
    ")\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bcb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
