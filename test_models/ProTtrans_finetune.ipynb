{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ad7ce903122a4ae389adced24144e518": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f8c081d7d76042bb99d0b541f81e8db3",
              "IPY_MODEL_156b0180a43841cd80dd88e1df42cddc",
              "IPY_MODEL_11eb7ae0183943fe8021b9f64d7a133b"
            ],
            "layout": "IPY_MODEL_55a056756c8f40e88807c8c9e9d0eec3"
          }
        },
        "f8c081d7d76042bb99d0b541f81e8db3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79422f6b7b8045f9bcce7ddf10b40fe7",
            "placeholder": "​",
            "style": "IPY_MODEL_7de5ae06b8274d79ab60bfd1807a7c7f",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "156b0180a43841cd80dd88e1df42cddc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c401ac323f1409db2b91e5cb0811c6e",
            "max": 546,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0d40742c561a408cb93b34023aea96c6",
            "value": 546
          }
        },
        "11eb7ae0183943fe8021b9f64d7a133b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7539dba7aa864226a4a77610548b1d43",
            "placeholder": "​",
            "style": "IPY_MODEL_7e9d38128d844defbb4fa1584c0c4e40",
            "value": " 546/546 [00:00&lt;00:00, 19.0kB/s]"
          }
        },
        "55a056756c8f40e88807c8c9e9d0eec3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79422f6b7b8045f9bcce7ddf10b40fe7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7de5ae06b8274d79ab60bfd1807a7c7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c401ac323f1409db2b91e5cb0811c6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d40742c561a408cb93b34023aea96c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7539dba7aa864226a4a77610548b1d43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e9d38128d844defbb4fa1584c0c4e40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b348f413ecc4dd290f7eaffa1ed2c65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1f5871d600b14d72b3cc9e844aca5110",
              "IPY_MODEL_8ac5477e3d3b412d8de2ffdb10e734d7",
              "IPY_MODEL_d3433a0ec6274bc08debe707ca091a7e"
            ],
            "layout": "IPY_MODEL_4ee883107d1f447789b54b9011ebdd34"
          }
        },
        "1f5871d600b14d72b3cc9e844aca5110": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_901ba378b03d44d7a251977cf8192649",
            "placeholder": "​",
            "style": "IPY_MODEL_dceffea904354129bc5ecdff9aa8d632",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "8ac5477e3d3b412d8de2ffdb10e734d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_118ad142190842e5a2c5edb3f62dae8c",
            "max": 11275562724,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_05b3fdef44024880974f17ba69fb45fa",
            "value": 11275562724
          }
        },
        "d3433a0ec6274bc08debe707ca091a7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92a9fe6e1e0e4ed193c83cccee054b85",
            "placeholder": "​",
            "style": "IPY_MODEL_908758025c6e4f5083d2f00c41a63527",
            "value": " 11.3G/11.3G [01:46&lt;00:00, 85.4MB/s]"
          }
        },
        "4ee883107d1f447789b54b9011ebdd34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "901ba378b03d44d7a251977cf8192649": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dceffea904354129bc5ecdff9aa8d632": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "118ad142190842e5a2c5edb3f62dae8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05b3fdef44024880974f17ba69fb45fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "92a9fe6e1e0e4ed193c83cccee054b85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "908758025c6e4f5083d2f00c41a63527": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtxdCzNwHHCA",
        "outputId": "ec7772fa-8630-4606-d0a4-5e8876c48fa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Specify your path within Google Drive\n",
        "path = \"/content/drive/My Drive/ProTrans_Finetune\"  # Change \"YourFolderPath\" to your specific path\n",
        "\n",
        "# Check if the path exists, create it if it doesn't\n",
        "if not os.path.exists(path):\n",
        "    os.makedirs(path)"
      ],
      "metadata": {
        "id": "036AB85NJGdS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers evaluate datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAqE1rWhKAMY",
        "outputId": "2f468590-a267-43b2-ca9b-229a73607a71"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Collecting dill (from evaluate)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
            "Collecting multiprocess (from evaluate)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
            "Installing collected packages: safetensors, dill, responses, multiprocess, huggingface-hub, tokenizers, transformers, datasets, evaluate\n",
            "Successfully installed datasets-2.14.6 dill-0.3.7 evaluate-0.4.1 huggingface-hub-0.17.3 multiprocess-0.70.15 responses-0.18.0 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.35.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import dependencies\n",
        "import os.path\n",
        "os.chdir(\"/content/drive/My Drive/ProTrans_Finetune\")\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import copy\n",
        "\n",
        "import transformers, datasets\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "from transformers.models.t5.modeling_t5 import T5Config, T5PreTrainedModel, T5Stack\n",
        "from transformers.utils.model_parallel_utils import assert_device_map, get_device_map\n",
        "from transformers import T5EncoderModel, T5Tokenizer\n",
        "from transformers import TrainingArguments, Trainer, set_seed\n",
        "\n",
        "from evaluate import load\n",
        "from datasets import Dataset\n",
        "\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "from scipy import stats\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "hqxuzFNjHtY7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set environment variables to run Deepspeed from a notebook\n",
        "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
        "os.environ[\"MASTER_PORT\"] = \"9994\"  # modify if RuntimeError: Address already in use\n",
        "os.environ[\"RANK\"] = \"0\"\n",
        "os.environ[\"LOCAL_RANK\"] = \"0\"\n",
        "os.environ[\"WORLD_SIZE\"] = \"1\""
      ],
      "metadata": {
        "id": "P4Q-jafuKSVD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Torch version: \",torch.__version__)\n",
        "print(\"Cuda version: \",torch.version.cuda)\n",
        "print(\"Numpy version: \",np.__version__)\n",
        "print(\"Pandas version: \",pd.__version__)\n",
        "print(\"Transformers version: \",transformers.__version__)\n",
        "print(\"Datasets version: \",datasets.__version__)"
      ],
      "metadata": {
        "id": "UtS73w8VKh68",
        "outputId": "d1fa105d-a8d8-494d-8fcc-22c0ab610148",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch version:  2.1.0+cu118\n",
            "Cuda version:  11.8\n",
            "Numpy version:  1.23.5\n",
            "Pandas version:  1.5.3\n",
            "Transformers version:  4.35.0\n",
            "Datasets version:  2.14.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load your CSV data into a pandas dataframe\n",
        "df = pd.read_csv(\"/content/ProTbert_finetune.csv\")\n",
        "\n",
        "# Optionally, if your dataset has a \"set\" column, you can drop the test set as in your example\n",
        "# df = df[df.set == \"train\"]\n",
        "\n",
        "# If your dataset does not already have a designated training and validation set, you can split it\n",
        "# Here, we're splitting the dataset with 80% for training and 20% for validation\n",
        "train_df, valid_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Rename the columns to 'sequence' and 'label', assuming your data has the appropriate columns.\n",
        "# Adjust the column names if necessary to match your CSV file's column headers\n",
        "train_df.columns = ['sequence', 'label'] + list(train_df.columns[2:])\n",
        "valid_df.columns = ['sequence', 'label'] + list(valid_df.columns[2:])\n",
        "\n",
        "# Keep only the necessary columns\n",
        "train_df = train_df[['sequence', 'label']]\n",
        "valid_df = valid_df[['sequence', 'label']]"
      ],
      "metadata": {
        "id": "SV4IgRJEKlmX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " my_train = pd.DataFrame(train_df)"
      ],
      "metadata": {
        "id": "CIqU0yZ7nJUC"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_valid = pd.DataFrame(valid_df)"
      ],
      "metadata": {
        "id": "4117hY2hnrKP"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modifies an existing transformer and introduce the LoRA layers\n",
        "\n",
        "class LoRAConfig:\n",
        "    def __init__(self):\n",
        "        self.lora_rank = 4\n",
        "        self.lora_init_scale = 0.01\n",
        "        self.lora_modules = \".*SelfAttention|.*EncDecAttention\"\n",
        "        self.lora_layers = \"q|k|v|o\"\n",
        "        self.trainable_param_names = \".*layer_norm.*|.*lora_[ab].*\"\n",
        "        self.lora_scaling_rank = 1\n",
        "        # lora_modules and lora_layers are speicified with regular expressions\n",
        "        # see https://www.w3schools.com/python/python_regex.asp for reference\n",
        "\n",
        "class LoRALinear(nn.Module):\n",
        "    def __init__(self, linear_layer, rank, scaling_rank, init_scale):\n",
        "        super().__init__()\n",
        "        self.in_features = linear_layer.in_features\n",
        "        self.out_features = linear_layer.out_features\n",
        "        self.rank = rank\n",
        "        self.scaling_rank = scaling_rank\n",
        "        self.weight = linear_layer.weight\n",
        "        self.bias = linear_layer.bias\n",
        "        if self.rank > 0:\n",
        "            self.lora_a = nn.Parameter(torch.randn(rank, linear_layer.in_features) * init_scale)\n",
        "            if init_scale < 0:\n",
        "                self.lora_b = nn.Parameter(torch.randn(linear_layer.out_features, rank) * init_scale)\n",
        "            else:\n",
        "                self.lora_b = nn.Parameter(torch.zeros(linear_layer.out_features, rank))\n",
        "        if self.scaling_rank:\n",
        "            self.multi_lora_a = nn.Parameter(\n",
        "                torch.ones(self.scaling_rank, linear_layer.in_features)\n",
        "                + torch.randn(self.scaling_rank, linear_layer.in_features) * init_scale\n",
        "            )\n",
        "            if init_scale < 0:\n",
        "                self.multi_lora_b = nn.Parameter(\n",
        "                    torch.ones(linear_layer.out_features, self.scaling_rank)\n",
        "                    + torch.randn(linear_layer.out_features, self.scaling_rank) * init_scale\n",
        "                )\n",
        "            else:\n",
        "                self.multi_lora_b = nn.Parameter(torch.ones(linear_layer.out_features, self.scaling_rank))\n",
        "\n",
        "    def forward(self, input):\n",
        "        if self.scaling_rank == 1 and self.rank == 0:\n",
        "            # parsimonious implementation for ia3 and lora scaling\n",
        "            if self.multi_lora_a.requires_grad:\n",
        "                hidden = F.linear((input * self.multi_lora_a.flatten()), self.weight, self.bias)\n",
        "            else:\n",
        "                hidden = F.linear(input, self.weight, self.bias)\n",
        "            if self.multi_lora_b.requires_grad:\n",
        "                hidden = hidden * self.multi_lora_b.flatten()\n",
        "            return hidden\n",
        "        else:\n",
        "            # general implementation for lora (adding and scaling)\n",
        "            weight = self.weight\n",
        "            if self.scaling_rank:\n",
        "                weight = weight * torch.matmul(self.multi_lora_b, self.multi_lora_a) / self.scaling_rank\n",
        "            if self.rank:\n",
        "                weight = weight + torch.matmul(self.lora_b, self.lora_a) / self.rank\n",
        "            return F.linear(input, weight, self.bias)\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return \"in_features={}, out_features={}, bias={}, rank={}, scaling_rank={}\".format(\n",
        "            self.in_features, self.out_features, self.bias is not None, self.rank, self.scaling_rank\n",
        "        )\n",
        "\n",
        "\n",
        "def modify_with_lora(transformer, config):\n",
        "    for m_name, module in dict(transformer.named_modules()).items():\n",
        "        if re.fullmatch(config.lora_modules, m_name):\n",
        "            for c_name, layer in dict(module.named_children()).items():\n",
        "                if re.fullmatch(config.lora_layers, c_name):\n",
        "                    assert isinstance(\n",
        "                        layer, nn.Linear\n",
        "                    ), f\"LoRA can only be applied to torch.nn.Linear, but {layer} is {type(layer)}.\"\n",
        "                    setattr(\n",
        "                        module,\n",
        "                        c_name,\n",
        "                        LoRALinear(layer, config.lora_rank, config.lora_scaling_rank, config.lora_init_scale),\n",
        "                    )\n",
        "    return transformer"
      ],
      "metadata": {
        "id": "PPiPoZhln6cf"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassConfig:\n",
        "    def __init__(self, dropout=0.2, num_labels=1):\n",
        "        self.dropout_rate = dropout\n",
        "        self.num_labels = num_labels\n",
        "\n",
        "class T5EncoderClassificationHead(nn.Module):\n",
        "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
        "\n",
        "    def __init__(self, config, class_config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.dropout = nn.Dropout(class_config.dropout_rate)\n",
        "        self.out_proj = nn.Linear(config.hidden_size, class_config.num_labels)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "\n",
        "        hidden_states =  torch.mean(hidden_states,dim=1)  # avg embedding\n",
        "\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = torch.tanh(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.out_proj(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "class T5EncoderForSimpleSequenceClassification(T5PreTrainedModel):\n",
        "\n",
        "    def __init__(self, config: T5Config, class_config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = class_config.num_labels\n",
        "        self.config = config\n",
        "\n",
        "        self.shared = nn.Embedding(config.vocab_size, config.d_model)\n",
        "\n",
        "        encoder_config = copy.deepcopy(config)\n",
        "        encoder_config.use_cache = False\n",
        "        encoder_config.is_encoder_decoder = False\n",
        "        self.encoder = T5Stack(encoder_config, self.shared)\n",
        "\n",
        "        self.dropout = nn.Dropout(class_config.dropout_rate)\n",
        "        self.classifier = T5EncoderClassificationHead(config, class_config)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "        # Model parallel\n",
        "        self.model_parallel = False\n",
        "        self.device_map = None\n",
        "\n",
        "    def parallelize(self, device_map=None):\n",
        "        self.device_map = (\n",
        "            get_device_map(len(self.encoder.block), range(torch.cuda.device_count()))\n",
        "            if device_map is None\n",
        "            else device_map\n",
        "        )\n",
        "        assert_device_map(self.device_map, len(self.encoder.block))\n",
        "        self.encoder.parallelize(self.device_map)\n",
        "        self.classifier = self.classifier.to(self.encoder.first_device)\n",
        "        self.model_parallel = True\n",
        "\n",
        "    def deparallelize(self):\n",
        "        self.encoder.deparallelize()\n",
        "        self.encoder = self.encoder.to(\"cpu\")\n",
        "        self.model_parallel = False\n",
        "        self.device_map = None\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.shared\n",
        "\n",
        "    def set_input_embeddings(self, new_embeddings):\n",
        "        self.shared = new_embeddings\n",
        "        self.encoder.set_input_embeddings(new_embeddings)\n",
        "\n",
        "    def get_encoder(self):\n",
        "        return self.encoder\n",
        "\n",
        "    def _prune_heads(self, heads_to_prune):\n",
        "        \"\"\"\n",
        "        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n",
        "        class PreTrainedModel\n",
        "        \"\"\"\n",
        "        for layer, heads in heads_to_prune.items():\n",
        "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.encoder(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            head_mask=head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        hidden_states = outputs[0]\n",
        "        logits = self.classifier(hidden_states)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            if self.config.problem_type is None:\n",
        "                if self.num_labels == 1:\n",
        "                    self.config.problem_type = \"regression\"\n",
        "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
        "                    self.config.problem_type = \"single_label_classification\"\n",
        "                else:\n",
        "                    self.config.problem_type = \"multi_label_classification\"\n",
        "\n",
        "            if self.config.problem_type == \"regression\":\n",
        "                loss_fct = MSELoss()\n",
        "                if self.num_labels == 1:\n",
        "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
        "                else:\n",
        "                    loss = loss_fct(logits, labels)\n",
        "            elif self.config.problem_type == \"single_label_classification\":\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            elif self.config.problem_type == \"multi_label_classification\":\n",
        "                loss_fct = BCEWithLogitsLoss()\n",
        "                loss = loss_fct(logits, labels)\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[1:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return SequenceClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )"
      ],
      "metadata": {
        "id": "FCFzZoK-pUmY"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def PT5_classification_model(num_labels):\n",
        "    # Load PT5 and tokenizer\n",
        "    model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\")\n",
        "    tokenizer = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\")\n",
        "\n",
        "    # Create new Classifier model with PT5 dimensions\n",
        "    class_config=ClassConfig(num_labels=num_labels)\n",
        "    class_model=T5EncoderForSimpleSequenceClassification(model.config,class_config)\n",
        "\n",
        "    # Set encoder and embedding weights to checkpoint weights\n",
        "    class_model.shared=model.shared\n",
        "    class_model.encoder=model.encoder\n",
        "\n",
        "    # Delete the checkpoint model\n",
        "    model=class_model\n",
        "    del class_model\n",
        "\n",
        "    # Print number of trainable parameters\n",
        "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "    print(\"ProtT5_Classfier\\nTrainable Parameter: \"+ str(params))\n",
        "\n",
        "    # Add model modification lora\n",
        "    config = LoRAConfig()\n",
        "\n",
        "    # Add LoRA layers\n",
        "    model = modify_with_lora(model, config)\n",
        "\n",
        "    # Freeze Embeddings and Encoder (except LoRA)\n",
        "    for (param_name, param) in model.shared.named_parameters():\n",
        "                param.requires_grad = False\n",
        "    for (param_name, param) in model.encoder.named_parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    for (param_name, param) in model.named_parameters():\n",
        "            if re.fullmatch(config.trainable_param_names, param_name):\n",
        "                param.requires_grad = True\n",
        "\n",
        "    # Print trainable Parameter\n",
        "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "    print(\"ProtT5_LoRA_Classfier\\nTrainable Parameter: \"+ str(params) + \"\\n\")\n",
        "\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "pTlEfix-ppY3"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Deepspeed config for optimizer CPU offload\n",
        "\n",
        "ds_config = {\n",
        "    \"fp16\": {\n",
        "        \"enabled\": \"auto\",\n",
        "        \"loss_scale\": 0,\n",
        "        \"loss_scale_window\": 1000,\n",
        "        \"initial_scale_power\": 16,\n",
        "        \"hysteresis\": 2,\n",
        "        \"min_loss_scale\": 1\n",
        "    },\n",
        "\n",
        "    \"optimizer\": {\n",
        "        \"type\": \"AdamW\",\n",
        "        \"params\": {\n",
        "            \"lr\": \"auto\",\n",
        "            \"betas\": \"auto\",\n",
        "            \"eps\": \"auto\",\n",
        "            \"weight_decay\": \"auto\"\n",
        "        }\n",
        "    },\n",
        "\n",
        "    \"scheduler\": {\n",
        "        \"type\": \"WarmupLR\",\n",
        "        \"params\": {\n",
        "            \"warmup_min_lr\": \"auto\",\n",
        "            \"warmup_max_lr\": \"auto\",\n",
        "            \"warmup_num_steps\": \"auto\"\n",
        "        }\n",
        "    },\n",
        "\n",
        "    \"zero_optimization\": {\n",
        "        \"stage\": 2,\n",
        "        \"offload_optimizer\": {\n",
        "            \"device\": \"cpu\",\n",
        "            \"pin_memory\": True\n",
        "        },\n",
        "        \"allgather_partitions\": True,\n",
        "        \"allgather_bucket_size\": 2e8,\n",
        "        \"overlap_comm\": True,\n",
        "        \"reduce_scatter\": True,\n",
        "        \"reduce_bucket_size\": 2e8,\n",
        "        \"contiguous_gradients\": True\n",
        "    },\n",
        "\n",
        "    \"gradient_accumulation_steps\": \"auto\",\n",
        "    \"gradient_clipping\": \"auto\",\n",
        "    \"steps_per_print\": 2000,\n",
        "    \"train_batch_size\": \"auto\",\n",
        "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
        "    \"wall_clock_breakdown\": False\n",
        "}"
      ],
      "metadata": {
        "id": "AXgTUEgurYuA"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seeds for reproducibility of your trainings run\n",
        "def set_seeds(s):\n",
        "    torch.manual_seed(s)\n",
        "    np.random.seed(s)\n",
        "    random.seed(s)\n",
        "    set_seed(s)\n",
        "\n",
        "# Dataset creation\n",
        "def create_dataset(tokenizer,seqs,labels):\n",
        "    tokenized = tokenizer(seqs, max_length=1024, padding=True, truncation=True)\n",
        "    dataset = Dataset.from_dict(tokenized)\n",
        "    dataset = dataset.add_column(\"labels\", labels)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# Main training fuction\n",
        "def train_per_protein(\n",
        "        train_df,         #training data\n",
        "        valid_df,         #validation data\n",
        "        num_labels= 1,    #1 for regression, >1 for classification\n",
        "\n",
        "        # effective training batch size is batch * accum\n",
        "        # we recommend an effective batch size of 8\n",
        "        batch= 4,         #for training\n",
        "        accum= 2,         #gradient accumulation\n",
        "\n",
        "        val_batch = 16,   #batch size for evaluation\n",
        "        epochs= 10,       #training epochs\n",
        "        lr= 3e-4,         #recommended learning rate\n",
        "        seed= 42,         #random seed\n",
        "        deepspeed= True,  #if gpu is large enough disable deepspeed for training speedup\n",
        "        gpu= 1 ):         #gpu selection (1 for first gpu)\n",
        "\n",
        "    # Set gpu device\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(gpu-1)\n",
        "\n",
        "    # Set all random seeds\n",
        "    set_seeds(seed)\n",
        "\n",
        "    # load model\n",
        "    model, tokenizer = PT5_classification_model(num_labels=num_labels)\n",
        "\n",
        "    # Preprocess inputs\n",
        "    # Replace uncommon AAs with \"X\"\n",
        "    train_df[\"sequence\"]=train_df[\"sequence\"].str.replace('|'.join([\"O\",\"B\",\"U\",\"Z\"]),\"X\",regex=True)\n",
        "    valid_df[\"sequence\"]=valid_df[\"sequence\"].str.replace('|'.join([\"O\",\"B\",\"U\",\"Z\"]),\"X\",regex=True)\n",
        "    # Add spaces between each amino acid for PT5 to correctly use them\n",
        "    train_df['sequence']=train_df.apply(lambda row : \" \".join(row[\"sequence\"]), axis = 1)\n",
        "    valid_df['sequence']=valid_df.apply(lambda row : \" \".join(row[\"sequence\"]), axis = 1)\n",
        "\n",
        "    # Create Datasets\n",
        "    train_set=create_dataset(tokenizer,list(train_df['sequence']),list(train_df['label']))\n",
        "    valid_set=create_dataset(tokenizer,list(valid_df['sequence']),list(valid_df['label']))\n",
        "\n",
        "    # Huggingface Trainer arguments\n",
        "    args = TrainingArguments(\n",
        "        \"./\",\n",
        "        evaluation_strategy = \"epoch\",\n",
        "        logging_strategy = \"epoch\",\n",
        "        save_strategy = \"no\",\n",
        "        learning_rate=lr,\n",
        "        per_device_train_batch_size=batch,\n",
        "        per_device_eval_batch_size=val_batch,\n",
        "        gradient_accumulation_steps=accum,\n",
        "        num_train_epochs=epochs,\n",
        "        seed = seed,\n",
        "        deepspeed= ds_config if deepspeed else None,\n",
        "    )\n",
        "\n",
        "    # Metric definition for validation data\n",
        "    def compute_metrics(eval_pred):\n",
        "        if num_labels>1:  # for classification\n",
        "            metric = load(\"accuracy\")\n",
        "            predictions, labels = eval_pred\n",
        "            predictions = np.argmax(predictions, axis=1)\n",
        "        else:  # for regression\n",
        "            metric = load(\"spearmanr\")\n",
        "            predictions, labels = eval_pred\n",
        "\n",
        "        return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "    # Trainer\n",
        "    trainer = Trainer(\n",
        "        model,\n",
        "        args,\n",
        "        train_dataset=train_set,\n",
        "        eval_dataset=valid_set,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    trainer.train()\n",
        "\n",
        "    return tokenizer, model, trainer.state.log_history"
      ],
      "metadata": {
        "id": "i9IfeTCtrd5F"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer, model, history = train_per_protein(my_train, my_valid, num_labels=1, batch=1, accum=8, epochs=20, seed=42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "ad7ce903122a4ae389adced24144e518",
            "f8c081d7d76042bb99d0b541f81e8db3",
            "156b0180a43841cd80dd88e1df42cddc",
            "11eb7ae0183943fe8021b9f64d7a133b",
            "55a056756c8f40e88807c8c9e9d0eec3",
            "79422f6b7b8045f9bcce7ddf10b40fe7",
            "7de5ae06b8274d79ab60bfd1807a7c7f",
            "5c401ac323f1409db2b91e5cb0811c6e",
            "0d40742c561a408cb93b34023aea96c6",
            "7539dba7aa864226a4a77610548b1d43",
            "7e9d38128d844defbb4fa1584c0c4e40",
            "7b348f413ecc4dd290f7eaffa1ed2c65",
            "1f5871d600b14d72b3cc9e844aca5110",
            "8ac5477e3d3b412d8de2ffdb10e734d7",
            "d3433a0ec6274bc08debe707ca091a7e",
            "4ee883107d1f447789b54b9011ebdd34",
            "901ba378b03d44d7a251977cf8192649",
            "dceffea904354129bc5ecdff9aa8d632",
            "118ad142190842e5a2c5edb3f62dae8c",
            "05b3fdef44024880974f17ba69fb45fa",
            "92a9fe6e1e0e4ed193c83cccee054b85",
            "908758025c6e4f5083d2f00c41a63527"
          ]
        },
        "id": "tJvjThJarnMk",
        "outputId": "f2f1e723-c5c3-4c1c-f1ba-6602038ac6f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/546 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad7ce903122a4ae389adced24144e518"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/11.3G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b348f413ecc4dd290f7eaffa1ed2c65"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get loss, val_loss, and the computed metric from history\n",
        "loss = [x['loss'] for x in history if 'loss' in x]\n",
        "val_loss = [x['eval_loss'] for x in history if 'eval_loss' in x]\n",
        "\n",
        "# Get spearman (for regression) or accuracy value (for classification)\n",
        "if [x['eval_spearmanr'] for x in history if 'eval_spearmanr' in x] != []:\n",
        "    metric = [x['eval_spearmanr'] for x in history if 'eval_spearmanr' in x]\n",
        "else:\n",
        "    metric = [x['eval_accuracy'] for x in history if 'eval_accuracy' in x]\n",
        "\n",
        "epochs = [x['epoch'] for x in history if 'loss' in x]\n",
        "\n",
        "# Create a figure with two y-axes\n",
        "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "# Plot loss and val_loss on the first y-axis\n",
        "line1 = ax1.plot(epochs, loss, label='train_loss')\n",
        "line2 = ax1.plot(epochs, val_loss, label='val_loss')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "\n",
        "# Plot the computed metric on the second y-axis\n",
        "line3 = ax2.plot(epochs, metric, color='red', label='val_metric')\n",
        "ax2.set_ylabel('Metric')\n",
        "ax2.set_ylim([0, 1])\n",
        "\n",
        "# Combine the lines from both y-axes and create a single legend\n",
        "lines = line1 + line2 + line3\n",
        "labels = [line.get_label() for line in lines]\n",
        "ax1.legend(lines, labels, loc='lower left')\n",
        "\n",
        "# Show the plot\n",
        "plt.title(\"Training History\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rQ9AGpRSrsDS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}